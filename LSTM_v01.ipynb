{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n",
      "IPython: 6.1.0\n",
      "numpy: 1.13.3\n",
      "scipy: 0.19.1\n",
      "pandas: 0.20.3\n",
      "seaborn 0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import der Libraries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "print('Python version:', sys.version)\n",
    "\n",
    "import os, re, csv, codecs\n",
    "\n",
    "import IPython\n",
    "print('IPython:', IPython.__version__)\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "print('numpy:', np.__version__)\n",
    "\n",
    "import scipy\n",
    "print('scipy:', scipy.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas:', pd.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#print('matplotlib:', plt.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print('seaborn', sns.__version__)\n",
    "\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data_train = pd.read_csv('D:/Projekte/Toxicity NLP/Data/train.csv')\n",
    "data_test = pd.read_csv('D:/Projekte/Toxicity NLP/Data/test.csv')\n",
    "\n",
    "# Definiere Features (X) und Labels (y)\n",
    "X = data_train['comment_text']\n",
    "y = data_train['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of the word corpus\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Häufigkeit der Wörter im dictionary | Auskommentiert aufgrund der Länge\n",
    "#tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Index der Wörter | Auskommentiert aufgrund der Länge\n",
    "#tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anders als bei den bisher vorgestellten klassischen ML-Modellen ist die Länge des Inputs eines neuronalen Netzes immer konstant, da sich die Anzahl der Input-Neuronen nicht ändert. Die Anzahl an Wörtern jedes Kommentars kann aber unterschiedlich sein. Daher verwendet man \"padding\", bei welchem jedes Kommentar auf die gleiche Länge getrimmt wird. Bei weniger Worten als maxlen wird mit Nullen aufgefüllt, bei mehr Worten als maxlen wird abgeschnitten.\n",
    "\n",
    "Um eine geeignete Länge zu finden, kann man sich die Verteilung der Wortanzahl anschauen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB0lS5/8DL/b9JFWJ7woAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e03fa0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verteilung der Länge der Kommentare\n",
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\n",
    "\n",
    "plt.hist(totalNumWords,bins = np.arange(0,410,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Histogram zeigt, dass die meisten Kommentare etwa 30 Wörter beinhalten. Wählt man die Länge des Inputs bei 200, hat man die Verteilung großzügig abgedeckt. Mit diesem Parameter kann natürlich beliebig experimentiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input Layer mit maxlen Neuronen\n",
    "inp = Input(shape=(maxlen, ))\n",
    "\n",
    "# Create Word-Embeddings\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM Layer\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape 3D tensor from embedding layer (None, 200, 128) into 2D tensor || Pooling Layer\n",
    "x = GlobalMaxPool1D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dropout layer to increase Generalization and prevent overfitting\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dense Layer with ReLu activation\n",
    "x = Dense(50, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Dropout layer\n",
    "x = Dropout(0.1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sigmoid Layer for binary classification\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 714s 5ms/step - loss: 0.1291 - acc: 0.9542 - val_loss: 0.1025 - val_acc: 0.9622\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 719s 5ms/step - loss: 0.0869 - acc: 0.9671 - val_loss: 0.1012 - val_acc: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18e033e8b70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lernparameter\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Training\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Modell erzielt bei der binären Klassifikation \"toxisch-nicht toxisch\" auf Anhieb respektable Ergebnisse und schlägt die klassischen Ansätze im Bezug auf die Accuracy. Zum Vergleich: der Voting-Classifier der Ensemble Methoden erzielte 95,40% Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 200, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 2,608,461\n",
      "Trainable params: 2,608,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Zusammenfassung des Modells\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell mit allen Klassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend wird das gleiche LSTM Modell mit allen sechs Klassen ausgeführt um der ursprünglichen Aufgabe zu entsprechen. Dazu werden alle Klassen im Output y festgehalten und der Output Layer auf 6 Neuronen erhöht (One-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinzufügen der Klassen\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = data_train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 689s 5ms/step - loss: 0.0682 - acc: 0.9776 - val_loss: 0.0501 - val_acc: 0.9818\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 686s 5ms/step - loss: 0.0446 - acc: 0.9833 - val_loss: 0.0461 - val_acc: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b3194bfda0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Input Layer mit maxlen Neuronen\n",
    "inp = Input(shape=(maxlen, ))\n",
    "\n",
    "# Create Word-Embeddings\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "\n",
    "# Create LSTM Layer\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(x)\n",
    "\n",
    "# Reshape 3D tensor from embedding layer (None, 200, 128) into 2D tensor || Pooling Layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "# Create Dropout layer to increase Generalization and prevent overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Create Dense Layer with ReLu activation\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "# Another Dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Create Sigmoid Layer for binary classification\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Initialize the Model\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Lernparameter\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Training\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich mit Multiclass LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for class toxic is 0.9509036278482083\n",
      "CV score for class severe_toxic is 0.9561873950478931\n",
      "CV score for class obscene is 0.9608835153632063\n",
      "CV score for class threat is 0.9520976080671062\n",
      "CV score for class insult is 0.9429496486501124\n",
      "CV score for class identity_hate is 0.9126526299428998\n",
      "Total CV score is 0.9459457374865711\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics, ensemble\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "clf_lr = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),\n",
    "                    ('clf', LogisticRegression(C=10, random_state=1))\n",
    "                    ])\n",
    "\n",
    "scores = []\n",
    "for label in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
    "    y = data_train[label]\n",
    "    cv_score = np.mean(ms.cross_val_score(clf_lr, X, y, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score)\n",
    "    print(f'CV score for class {label} is {cv_score}')\n",
    "    \n",
    "    \n",
    "print(f'Total CV score is {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch hier schneidet das LSTM Netz deutlich besser ab, als die Multiclass LR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ergänzungen zum Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering mit Verhältnis der Großbuchstaben im Kommentar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausbau der Daten mit zusätzlichen Features Länge des Kommentars, Anzahl an Großbuchstaben und Großbuchstaben/Länge\n",
    "data_train['total_length'] = data_train['comment_text'].apply(len)\n",
    "data_train['capitals'] = data_train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "data_train['caps_ratio'] = data_train.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinzufügen des Features zum Inputarray\n",
    "data_array = np.array(data_train['caps_ratio'])\n",
    "X_t_extended = np.hstack((X_t, np.atleast_2d(data_array).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 699s 5ms/step - loss: 0.0677 - acc: 0.9781 - val_loss: 0.0494 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 696s 5ms/step - loss: 0.0446 - acc: 0.9835 - val_loss: 0.0465 - val_acc: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b31aff1da0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hinzufügen der Klassen\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = data_train[list_classes].values\n",
    "\n",
    "# Define Input Layer mit maxlen Neuronen\n",
    "inp = Input(shape=(201, ))\n",
    "\n",
    "# Create Word-Embeddings\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "\n",
    "# Create LSTM Layer\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(x)\n",
    "\n",
    "# Reshape 3D tensor from embedding layer (None, 200, 128) into 2D tensor || Pooling Layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "\n",
    "# Create Dropout layer to increase Generalization and prevent overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Create Dense Layer with ReLu activation\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "# Another Dropout layer\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Create Sigmoid Layer for binary classification\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Initialize the Model\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Lernparameter\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Training\n",
    "model.fit(X_t_extended, y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verbesserung ist leider nur unerheblich und könnte wohl genauso gut zufällig sein. Die Vermutung liegt nahe, dass ein zusätzliches Feature bei 200 Inputs nicht allzu stark ins Gewicht fällt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
