{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]\n",
      "IPython: 6.1.0\n",
      "numpy: 1.13.3\n",
      "scipy: 0.19.1\n",
      "pandas: 0.20.3\n",
      "scikit-learn: 0.19.1\n",
      "seaborn 0.8.0\n"
     ]
    }
   ],
   "source": [
    "#Import der Libraries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "print('Python version:', sys.version)\n",
    "\n",
    "import IPython\n",
    "print('IPython:', IPython.__version__)\n",
    "\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "print('numpy:', np.__version__)\n",
    "\n",
    "import scipy\n",
    "print('scipy:', scipy.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print('pandas:', pd.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#print('matplotlib:', plt.__version__)\n",
    "\n",
    "import sklearn\n",
    "print('scikit-learn:', sklearn.__version__)\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "print('seaborn', sns.__version__)\n",
    "\n",
    "import nltk\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten einlesen\n",
    "data_train = pd.read_csv('D:/Projekte/Toxicity NLP/Data/train.csv')\n",
    "data_test = pd.read_csv('D:/Projekte/Toxicity NLP/Data/test.csv')\n",
    "\n",
    "# Definiere Features (X) und Labels (y)\n",
    "X = data_train['comment_text']\n",
    "y = data_train['toxic']\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einzelne Modelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachfolgend werden die einzelnen Modelle mit GridSearch trainiert um geeignete Hyperparamter zu finden. Cross-Validation wird zusätzlich verwendet um die Robustheit der Ergebnisse sicherzustellen. Alle GridSearches werden jedoch unter Beachtung des Berechnungsaufwands und RAM-Bedarfs durchgeführt. Das Hinzufügen eines zusätzlichen Parameters im GridSearch erhöht die Anzahl der Fits um Faktor 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 98.25s\n",
      "\n",
      "Best score: 0.9503355872934305\n",
      "Best parameters set: {'clf__alpha': 0.66666666666666663}\n"
     ]
    }
   ],
   "source": [
    "# MNB\n",
    "clf_mnb = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),\n",
    "                    ('clf', MultinomialNB())\n",
    "                    ])\n",
    "params = {\n",
    "    'clf__alpha':np.linspace(0.0, 1.0, 10),\n",
    "}\n",
    "\n",
    "grid_search = ms.GridSearchCV(clf_mnb, params,n_jobs=-1, verbose=1, cv=3)\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "t1 = time() - t0\n",
    "print(f\"done in {t1:.2f}s\")\n",
    "print()\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters set: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  2.4min finished\n",
      "D:\\Software\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 167.37s\n",
      "\n",
      "Best score: 0.9524788338733229\n",
      "Best parameters set: {'clf__C': 10}\n"
     ]
    }
   ],
   "source": [
    "# Logistische Regression\n",
    "clf_lr = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),\n",
    "                    ('clf', LogisticRegression(random_state=1, n_jobs=-1))\n",
    "                    ])\n",
    "params = {\n",
    "    #'clf__penalty':('l1', 'l2'),\n",
    "    'clf__C': (0.01, 0.1, 1, 10, 100),\n",
    "    \n",
    "}\n",
    "\n",
    "grid_search = ms.GridSearchCV(clf_lr, params,n_jobs=-1, verbose=1, cv=3)\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "t1 = time() - t0\n",
    "print(f\"done in {t1:.2f}s\")\n",
    "print()\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters set: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:  1.4min remaining:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 215.62s\n",
      "\n",
      "Best score: 0.9426211529663912\n",
      "Best parameters set:\n",
      "{'clf__max_depth': 15, 'clf__n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest mit Grid Search\n",
    "clf_rf = Pipeline([\n",
    "    ('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),\n",
    "    ('clf', ensemble.RandomForestClassifier(criterion='entropy',min_samples_leaf=5, random_state=1, max_features=None)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "   'clf__n_estimators': (5, 10),\n",
    "   'clf__max_depth': (5, 10, 15),\n",
    "   #'clf__criterion' : ('entropy', 'gini'),\n",
    "}\n",
    "\n",
    "grid_search = ms.GridSearchCV(clf_rf, params,n_jobs=-1, verbose=1, cv=2)\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "t1 = time() - t0\n",
    "print(f\"done in {t1:.2f}s\")\n",
    "print()\n",
    "\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters set: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Präzision des Random Forest könnte durch Freigabe der limitierenden Parameter zusätzlich erhöht werden. Allerdings steigt die Berechnungsdauer stark an und mein älterer 8GB RAM wird leider gesprengt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 334.55s\n",
      "\n",
      "Best score: 0.9456981531731956\n",
      "Best parameters set:\n",
      "Best parameters set: {'clf_ada__n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "# RF mit Adaboost\n",
    "clf_rf = ensemble.RandomForestClassifier(n_estimators=10,max_depth=15,criterion=\"entropy\",max_features=None,random_state=1,n_jobs=-1)\n",
    "\n",
    "params = {\n",
    "    'clf_ada__n_estimators' : (1, 2, 5),\n",
    "}\n",
    "\n",
    "text_clf6 = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),\n",
    "                    ('clf_ada', ensemble.AdaBoostClassifier(clf_rf, n_estimators=5, random_state=1))\n",
    "                     ])\n",
    "grid_search = ms.GridSearchCV(text_clf6, params, n_jobs=-1, verbose=1, cv=2)\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "t1 = time() - t0\n",
    "print(f\"done in {t1:.2f}s\")\n",
    "print()\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(\"Best parameters set:\")\n",
    "print(f\"Best parameters set: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble mit VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Ensemble werden die heterogenen Modelle MNB (bessere Performance als BNB), Logistic Regression und Random Forest ausgewählt. Die Modelle sollen sich in ihrer Vorhersagemethodik unterscheiden und möglichst unabhängig voneinander sein. Dadurch werden im Ensemble die spezifischen Schwächen jedes Modells ausgeglichen. Daher wird auf ähnliche Varianten zur Ensemblebildung verzichtet. Dazu zählen z.B. MNB vs BNB und LR vs SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9498: [Multinomial Naive Bayes]\n",
      "Accuracy: 0.9529: [Logistic Regression]\n",
      "Accuracy: 0.9463: [Random Forest m. Adaboost]\n",
      "Accuracy: 0.9540: [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "# Modelle mit den besten Parametern aus den GridSearches\n",
    "clf_mnb = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),('clf', MultinomialNB(alpha = 0.667))])\n",
    "clf_lr = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),('clf', LogisticRegression(C=10))])\n",
    "clf_rf = ensemble.RandomForestClassifier(n_estimators=10,max_depth=15,min_samples_leaf=5,criterion=\"entropy\",max_features=None,random_state=1,n_jobs=-1)\n",
    "clf_rf_ada = Pipeline([('vect', CountVectorizer(token_pattern='[a-zA-Z\\'][a-zA-Z\\']+', stop_words='english')),('clf', ensemble.AdaBoostClassifier(clf_rf, n_estimators=5, random_state=1))])\n",
    "\n",
    "est = [('mnb', clf_mnb),\n",
    "        ('lr', clf_lr),\n",
    "        ('rf_ada', clf_rf_ada)\n",
    "]\n",
    "\n",
    "vclf = ensemble.VotingClassifier(estimators=est, n_jobs=-1, voting='hard')\n",
    "\n",
    "for clf, label in zip([clf_mnb, clf_lr, clf_rf_ada, vclf], ['Multinomial Naive Bayes', 'Logistic Regression', 'Random Forest m. Adaboost', 'Ensemble']):\n",
    "    scores = ms.cross_val_score(clf, X, y, cv=2, scoring='accuracy')\n",
    "    print(f\"Accuracy: {scores.mean():.4f}: [{label}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Methodenensemble erzielt damit eine leichte Verbesserung gegenüber den Einzelmethoden. Eine größere Verbesserung könnte durch Verwendung von weiteren Modellen erreicht werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
